# ğŸŸ OpenClaw LLM Router â€” Release Notes v1.1.0

> ğŸ¤ *Built on top of [alexrudloff/llmrouter](https://github.com/alexrudloff/llmrouter) â€” thanks Alex for the excellent foundation!*

---

## ğŸš€ What's New in v1.1.0

### ğŸ”€ Automatic Failover Chains

The LLM Router now supports **multiple models per complexity level**. If one model is unavailable, it automatically switches to the next in the list â€” no manual intervention required!

| Before | After |
|--------|-------|
| Single model per tier | Priority list with automatic fallback |
| Manual provider switching | Seamless failover |
| Downtime on provider issues | Continuous operation |

**What this means for you:**
- âœ… **Higher reliability** â€” If a provider is down, your request still goes through
- ğŸ”„ **Flexible routing** â€” Combine local models (Exo, LM Studio) with cloud providers (Anthropic, Pollinations)
- âš¡ **No interruptions** â€” The switch happens automatically in the background

---

## âš™ï¸ How It Works

Instead of just one model per level, specify a **priority list**:

```yaml
models:
  super_easy:
    - "exo:mlx-community/GLM-4.7-Flash-6bit"      # ğŸ  1st: Local (free)
    - "lmstudio:zai-org/glm-4.7-flash"             # ğŸ  2nd: Local fallback
    - "anthropic:claude-haiku-4-5-20251001"        # â˜ï¸ 3rd: Cloud fallback
```

**Flow:**
1. ğŸ” Router classifies request (e.g., "super_easy")
2. ğŸ¯ Attempts Model 1 (Exo)
3. ğŸ”„ If Exo fails â†’ automatic attempt with Model 2 (LM Studio)
4. ğŸ”„ If LM Studio fails â†’ attempt with Model 3 (Anthropic)
5. âŒ Only when ALL models fail â†’ error returned

---

## ğŸ“‹ Example Configuration

```yaml
# config.yaml â€” Complete example with failover chains

models:
  super_easy:
    - "exo:mlx-community/GLM-4.7-Flash-6bit"      # ğŸš€ Fast & local
    - "lmstudio:zai-org/glm-4.7-flash"             # ğŸ  Local fallback
    - "anthropic:claude-haiku-4-5-20251001"        # â˜ï¸ Cloud fallback

  easy:
    - "exo:mlx-community/GLM-4.7-Flash-6bit"
    - "lmstudio:zai-org/glm-4.7-flash"
    - "anthropic:claude-haiku-4-5-20251001"

  medium:
    - "pollinations:glm"                           # ğŸ†“ Free tier
    - "pollinations:deepseek"                      # ğŸ”„ Alternative
    - "anthropic:claude-sonnet-4-20250514"         # ğŸ’ Premium fallback
    - "lmstudio:zai-org/glm-4.7-flash"
    - "exo:mlx-community/GLM-4.7-Flash-6bit"

  hard:
    - "pollinations:glm"
    - "pollinations:deepseek"
    - "anthropic:claude-sonnet-4-20250514"
    - "lmstudio:zai-org/glm-4.7-flash"
    - "exo:mlx-community/GLM-4.7-Flash-6bit"

  super_hard:
    - "anthropic:claude-opus-4-20250514"           # ğŸ‘‘ Best model
    - "pollinations:glm"                           # ğŸ”„ Fallback
    - "lmstudio:zai-org/glm-4.7-flash"
    - "exo:mlx-community/GLM-4.7-Flash-6bit"
```

---

## âš ï¸ Breaking Changes

**None!** ğŸ‰ The old syntax with single strings continues to work:

```yaml
# âœ¨ Old syntax (still works)
models:
  super_easy: "anthropic:claude-haiku-4-5-20251001"

# ğŸ†• New syntax (recommended for failover)
models:
  super_easy:
    - "anthropic:claude-haiku-4-5-20251001"
    - "exo:mlx-community/GLM-4.7-Flash-6bit"
```

**Note:** Tool calls with `model:` override still do **not** use failover â€” they use the explicitly specified model.

---

## â¬†ï¸ Upgrade Guide

### Step 1: Update Repository
```bash
cd /path/to/llmrouter
git pull origin main
```

### Step 2: Extend Config (Optional)
Edit `config.yaml` and add failover chains:

```yaml
# Before:
models:
  super_easy: "anthropic:claude-haiku-4-5-20251001"

# After:
models:
  super_easy:
    - "exo:mlx-community/GLM-4.7-Flash-6bit"      # ğŸ  Local first (free)
    - "anthropic:claude-haiku-4-5-20251001"        # â˜ï¸ Cloud fallback
```

### Step 3: Restart Server
```bash
# If running as a service
launchctl unload ~/Library/LaunchAgents/com.llmrouter.plist
launchctl load ~/Library/LaunchAgents/com.llmrouter.plist

# Or manually
python server.py --openclaw
```

---

## ğŸ› Bugfixes in This Release

| Issue | Fix |
|-------|-----|
| Crash on empty provider config | ğŸ›¡ï¸ Null-safe provider loading |
| Array config not supported | âœ… Full array/list support |
| First provider fail = total fail | ğŸ”„ Attempt all providers before error |

---

## ğŸ“Š Compatibility

| Component | Version |
|-----------|---------|
| ğŸ Python | 3.10+ |
| ğŸ¦ OpenClaw | Fully compatible |
| ğŸ”Œ Providers | All previous (Anthropic, OpenAI, Google, Kimi, Ollama, Exo, LM Studio, Pollinations) |

---

## ğŸ¯ Quick Links

- ğŸ“– [Full Documentation](README.md)
- âš™ï¸ [Example Config](config.yaml.example)
- ğŸ› [Report Issues](../../issues)

---

**Enjoy the more reliable routing!** ğŸ‰

*Release Date: 2026-02-13*  
*Tag: v1.1.0*  
*Built with ğŸŸ OpenClaw*
