# ğŸŸ OpenClaw LLM Router â€” Release Notes v1.1.0

> ğŸ¤ *Auf Basis von [alexrudloff/llmrouter](https://github.com/alexrudloff/llmrouter) â€” danke Alex fÃ¼r die exzellente Grundlage!*

---

## ğŸš€ Was ist neu in v1.1.0

### ğŸ”€ Automatische Failover-Chains

Der LLM Router unterstÃ¼tzt jetzt **mehrere Modelle pro KomplexitÃ¤tsstufe**. Wenn ein Modell nicht verfÃ¼gbar ist, wird automatisch zum nÃ¤chsten in der Liste gewechselt â€” ganz ohne manuellen Eingriff!

| Vorher | Nachher |
|--------|---------|
| Ein Modell pro Stufe | PrioritÃ¤tenliste mit automatischem Fallback |
| Manuelles Provider-Switching | Nahtloser Failover |
| Ausfallzeit bei Provider-Problemen | Kontinuierlicher Betrieb |

**Was bedeutet das fÃ¼r dich:**
- âœ… **HÃ¶here ZuverlÃ¤ssigkeit** â€” Wenn ein Provider down ist, lÃ¤uft deine Anfrage trotzdem durch
- ğŸ”„ **Flexibles Routing** â€” Kombiniere lokale Modelle (Exo, LM Studio) mit Cloud-Providern (Anthropic, Pollinations)
- âš¡ **Keine Unterbrechungen** â€” Der Wechsel passiert automatisch im Hintergrund

---

## âš™ï¸ Wie funktioniert es?

Statt nur einem Modell pro Stufe kannst du jetzt eine **PrioritÃ¤tenliste** angeben:

```yaml
models:
  super_easy:
    - "exo:mlx-community/GLM-4.7-Flash-6bit"      # ğŸ  1.: Lokal (kostenlos)
    - "lmstudio:zai-org/glm-4.7-flash"             # ğŸ  2.: Lokaler Fallback
    - "anthropic:claude-haiku-4-5-20251001"        # â˜ï¸ 3.: Cloud-Fallback
```

**Ablauf:**
1. ğŸ” Router klassifiziert Anfrage (z.B. "super_easy")
2. ğŸ¯ Versucht Modell 1 (Exo)
3. ğŸ”„ Wenn Exo fehlschlÃ¤gt â†’ automatischer Versuch mit Modell 2 (LM Studio)
4. ğŸ”„ Wenn LM Studio fehlschlÃ¤gt â†’ Versuch mit Modell 3 (Anthropic)
5. âŒ Erst wenn ALLE Modelle fehlschlagen â†’ Fehler zurÃ¼ckgegeben

---

## ğŸ“‹ Beispiel-Konfiguration

```yaml
# config.yaml â€” Komplettes Beispiel mit Failover-Chains

models:
  super_easy:
    - "exo:mlx-community/GLM-4.7-Flash-6bit"      # ğŸš€ Schnell & lokal
    - "lmstudio:zai-org/glm-4.7-flash"             # ğŸ  Lokaler Fallback
    - "anthropic:claude-haiku-4-5-20251001"        # â˜ï¸ Cloud-Fallback

  easy:
    - "exo:mlx-community/GLM-4.7-Flash-6bit"
    - "lmstudio:zai-org/glm-4.7-flash"
    - "anthropic:claude-haiku-4-5-20251001"

  medium:
    - "pollinations:glm"                           # ğŸ†“ Kostenlos
    - "pollinations:deepseek"                      # ğŸ”„ Alternative
    - "anthropic:claude-sonnet-4-20250514"         # ğŸ’ Premium-Fallback
    - "lmstudio:zai-org/glm-4.7-flash"
    - "exo:mlx-community/GLM-4.7-Flash-6bit"

  hard:
    - "pollinations:glm"
    - "pollinations:deepseek"
    - "anthropic:claude-sonnet-4-20250514"
    - "lmstudio:zai-org/glm-4.7-flash"
    - "exo:mlx-community/GLM-4.7-Flash-6bit"

  super_hard:
    - "anthropic:claude-opus-4-20250514"           # ğŸ‘‘ Bestes Modell
    - "pollinations:glm"                           # ğŸ”„ Fallback
    - "lmstudio:zai-org/glm-4.7-flash"
    - "exo:mlx-community/GLM-4.7-Flash-6bit"
```

---

## âš ï¸ Breaking Changes

**Keine!** ğŸ‰ Die alte Syntax mit einzelnem String funktioniert weiterhin:

```yaml
# âœ¨ Alte Syntax (funktioniert weiterhin)
models:
  super_easy: "anthropic:claude-haiku-4-5-20251001"

# ğŸ†• Neue Syntax (empfohlen fÃ¼r Failover)
models:
  super_easy:
    - "anthropic:claude-haiku-4-5-20251001"
    - "exo:mlx-community/GLM-4.7-Flash-6bit"
```

**Hinweis:** Tool-Calls mit `model:`-Override verwenden weiterhin **kein** Failover â€” sie nutzen explizit das angegebene Modell.

---

## â¬†ï¸ Upgrade-Anleitung

### Schritt 1: Repository aktualisieren
```bash
cd /path/to/llmrouter
git pull origin main
```

### Schritt 2: Config erweitern (optional)
Bearbeite `config.yaml` und fÃ¼ge Failover-Chains hinzu:

```yaml
# Vorher:
models:
  super_easy: "anthropic:claude-haiku-4-5-20251001"

# Nachher:
models:
  super_easy:
    - "exo:mlx-community/GLM-4.7-Flash-6bit"      # ğŸ  Lokales Modell zuerst (kostenlos)
    - "anthropic:claude-haiku-4-5-20251001"        # â˜ï¸ Cloud-Fallback
```

### Schritt 3: Server neu starten
```bash
# Falls als Service lÃ¤uft
launchctl unload ~/Library/LaunchAgents/com.llmrouter.plist
launchctl load ~/Library/LaunchAgents/com.llmrouter.plist

# Oder manuell
python server.py --openclaw
```

---

## ğŸ› Bugfixes in diesem Release

| Problem | Fix |
|---------|-----|
| Absturz bei leerer Provider-Konfiguration | ğŸ›¡ï¸ Null-safe Provider-Loading |
| Array-Config nicht unterstÃ¼tzt | âœ… Volle Array/List-UnterstÃ¼tzung |
| Erster Provider-Fail = Total-Fail | ğŸ”„ Alle Provider werden versucht |

---

## ğŸ“Š KompatibilitÃ¤t

| Komponente | Version |
|------------|---------|
| ğŸ Python | 3.10+ |
| ğŸ¦ OpenClaw | Voll kompatibel |
| ğŸ”Œ Provider | Alle bisherigen (Anthropic, OpenAI, Google, Kimi, Ollama, Exo, LM Studio, Pollinations) |

---

## ğŸ¯ Schnell-Links

- ğŸ“– [VollstÃ¤ndige Dokumentation](README.md)
- âš™ï¸ [Beispiel-Konfiguration](config.yaml.example)
- ğŸ› [Issues melden](../../issues)

---

**Viel SpaÃŸ mit dem zuverlÃ¤ssigeren Routing!** ğŸ‰

*Release-Datum: 2026-02-13*  
*Tag: v1.1.0*  
*Gebaut mit ğŸŸ OpenClaw*
